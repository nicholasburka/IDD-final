# Final Project
by Nicholas Burka (nab262)

and Jeongmin Huh (jh2229)

## Big Idea

We have a batch of old cell phones one of our team members purchased from a cell phone repair store that was going out of business (to convert to a music studio). We’d like to “revive” these, by using them as augmented displays and touchscreens for regular computer use. We hope to answer the questions: What typical HCI experiences would be enhanced by an additional, smaller screen? How can using the phone’s touch display augment the user experience?

We’ve thought of several potential use cases, scaling in anticipated complexity. However, at the basic level of what a screen display is, they best communicate images or video. Then we thought about how expansive the human visual experience becomes through the medium of a phone display, yet so limited by its small framed screen. So we came up with an idea that attempts to expand our attention to detail by bringing to the forefront the missed details of our perspective through the screens.


## Parts Needed
Batch of old phones to test
Raspberry Pi 4 + External Camera
(Photo)


Spacedesk (software) installed on the phone and computer
(Photo)


## Ideation phase
Initially, our idea was to use the phone for productivity purposes, by delegating minimal but useful information from the big screen to the smaller phone screens. This is how we first strived to develop "Digital Sticky Notes". Instead of using paper sticky notes around the deskspace, room, house, one would place these old phones and from the connected computer, compose messages, tasks, figures, or motivation quotes and send them to each phone. The reader of the messages on the phone screens would also be able to interact with these screens to communicate back through these screens with a few simple touches of the screen. However, we found this idea not interactive enough for our liking.

<img width="588" alt="image" src="https://user-images.githubusercontent.com/89954387/145902591-ab3ff7d7-b0c3-4d43-b33e-af3d0ad3fc43.png">


Our new idea is more suited for interactive purposes, by delegating segments of one image stream to however many screen devices connected to the host. This was done using MQTT Explorer. This opens up many possibilities for potential uses. The confusion caused by the breaking of the mechanism of how human process what s/he sees allows for interactive challenges that can explored in games, performance arts, or even film effects.

<img width="586" alt="image" src="https://user-images.githubusercontent.com/89954387/145922215-2d62b05f-8e9c-4558-b608-f7e0b9bc52aa.png">


## Storyboard

<img width="1025" alt="image" src="https://user-images.githubusercontent.com/89954387/145920015-ae853088-5a6b-41d3-84f4-04076a52273b.png">

This storyboard is inspired by the famous parable from India of the blind men and the elephant.
The illustration suggests that many visual effects could be accomplished for something as simple as puzzles for children, or grandiose for large-scale concerts, or in films, not unlike that achieved in the iconic Bruce Lee scene in the 1974 film "Enter the Dragon", which evokes a unique set of emotions (confusion, suspense, curiosity etc.) in the viewer.
![image](https://user-images.githubusercontent.com/89954387/145920336-aa8841d0-0d09-4108-a22c-48abde012543.png)


## Setup

MQTT Explorer (Photos)

Devices connected through spacedesk (Photos)

Coding (photos needed)


## Demo
Video during presentation
